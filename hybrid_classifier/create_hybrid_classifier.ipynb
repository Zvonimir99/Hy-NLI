{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the hybrid classifier (the script is written for hybridizing with BERT. Just replacce "BERT" with "XLNet" to get hybridization with XLNet.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing the training (or testing) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import metrics \n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open input and output files. The input files are the output files of the GKR4NLI system and the DL model.\n",
    "dlFile = open ('SICK_trial_and_train_BERT_results.csv', 'r')\n",
    "symbFile = open ('SICK_trial_and_train_GKR4NLI_results.csv', 'r')\n",
    "mergedFile = open ('SICK_trial_and_train_BERT_GKR4NLI_input_for_hybrid_classifier.csv', 'w')\n",
    "mergedFile.write(\"ID\\tComplexCtxs\\tContraFlag\\tVeridical\\tAntiveridical\\tAveridical\\tEquals\\tSuperclass\\tSubclass\\tDisjoint\\tTargetLabel\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files and initialize parameters\n",
    "dlLines = dlFile.readlines()\n",
    "symbLines = symbFile.readlines()\n",
    "# Create a dictionary holding the predicted label of the DL model. Will need it for evaluation as well.\n",
    "dict_of_dl_labels = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the dl file and store the predicted label of each pair in a dictionary.\n",
    "for line in dlLines:\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    elements = line.split(\"\\t\")\n",
    "    dict_of_dl_labels[elements[0]] = elements[1].replace(\"0\", \"E\").replace(\"1\", \"C\").replace(\"2\", \"N\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the symbolic file and compare the symbolic and the dl labels to gold and produce the final\n",
    "# target label that will be learned by the classifier. Produce a merged file with the features and the target label.\n",
    "for line in symbLines:\n",
    "    if line.startswith(\"pair_ID\"):\n",
    "        continue\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    elements = line.split(\"\\t\")\n",
    "    id_ = elements[0]\n",
    "    gold_label = elements[1]\n",
    "    features = elements[2:-1]\n",
    "    symb_label = elements[-1]\n",
    "    dl_label = dict_of_dl_labels[id_]\n",
    "    target_label = \"\"\n",
    "    if dl_label == gold_label and symb_label == gold_label:\n",
    "        target_label = \"B\"\n",
    "    elif dl_label == gold_label:\n",
    "        target_label = \"DL\"\n",
    "    elif symb_label == gold_label:\n",
    "        target_label = \"S\"\n",
    "    else:\n",
    "         target_label = \"N\"\n",
    "    mergedFile.write(id_+\"\\t\"+\"\\t\".join([str(f) for f in features])+\"\\t\" + target_label+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close all opened files.\n",
    "dlFile.close()\n",
    "symbFile.close()\n",
    "mergedFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lenght::  6213\n",
      "Dataset Shape::  (6213, 11)\n",
      "Dataset:: \n",
      "     ID  ComplexCtxs  ContraFlag  Veridical  Antiveridical  Averidical  \\\n",
      "0    4a            1           0          1              1           0   \n",
      "1    4b            0           0          1              0           0   \n",
      "2   24a            0           0          1              0           0   \n",
      "3  105a            0           0          1              0           0   \n",
      "4  116a            1           0          1              0           0   \n",
      "\n",
      "   Equals  Superclass  Subclass  Disjoint TargetLabel  \n",
      "0       1           0         0         0           B  \n",
      "1       1           0         0         0           B  \n",
      "2       1           1         0         0           B  \n",
      "3       1           1         0         0           B  \n",
      "4       0           1         0         0           B  \n"
     ]
    }
   ],
   "source": [
    "# Read training set that was created in the previous section.\n",
    "training_data = pd.read_csv('SICK_trial_and_train_BERT_GKR4NLI_input_for_hybrid_classifier.csv', sep= '\\t', header= 0) \n",
    "    \n",
    "print (\"Dataset Lenght:: \", len(training_data))\n",
    "print (\"Dataset Shape:: \", training_data.shape)\n",
    "\n",
    "print (\"Dataset:: \")\n",
    "print (training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B     4706\n",
       "DL    1234\n",
       "N      195\n",
       "S       78\n",
       "Name: TargetLabel, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how the labels are distributed in order to resample properly.\n",
    "dl = training_data[training_data.TargetLabel==\"DL\"]\n",
    "symbolic = training_data[training_data.TargetLabel==\"S\"]\n",
    "both = training_data[training_data.TargetLabel==\"B\"]\n",
    "none = training_data[training_data.TargetLabel==\"N\"]\n",
    "training_data['TargetLabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S     78\n",
       "B     78\n",
       "DL    60\n",
       "Name: TargetLabel, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the pairs with DL and B target labels have to be downsampled to match the much fewer instances of S labels\n",
    "# (SICK is a relatively easy corpus where most pairs can be solved equally well by the two approaches and the\n",
    "# rest of them can be mostly solved by the DL model.)\n",
    "dl_downsampled = resample(dl, \n",
    "                                replace=False,     # sample with replacement\n",
    "                               n_samples=60,    # to match minority class\n",
    "                             random_state=123) # reproducible results\n",
    "\n",
    "both_downsampled = resample(both, \n",
    "                                replace=True,     # sample with replacement\n",
    "                               n_samples=78,    # to match minority class\n",
    "                             random_state=123) # reproducible results\n",
    "\n",
    "\n",
    "resampled_set = pd.concat([symbolic,dl_downsampled, both_downsampled])\n",
    "resampled_set['TargetLabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "X_train = resampled_set.values[:, 1:-1]\n",
    "Y_train = resampled_set.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, hidden_layer_sizes=(8,), learning_rate='adaptive',\n",
       "              learning_rate_init=0.01, max_iter=1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an MLP classifier with the best parameters defined after a grid search.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(8,),\n",
    "                                       activation='relu',\n",
    "                                       solver='adam',\n",
    "                                       learning_rate='adaptive',\n",
    "                                       max_iter=1000,\n",
    "                                       learning_rate_init=0.01,\n",
    "                                       alpha=0.01)\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, read the test set to store the symbolic, DL and gold labels of the pairs\n",
    "\n",
    "# Open the files\n",
    "dlFile_TEST = open('SICK_test_BERT_results.csv', \"r\")\n",
    "symbFile_TEST = open('SICK_test_GKR4NLI_results.csv', \"r\")\n",
    "\n",
    "# Read the files\n",
    "dlLines_TEST = dlFile_TEST.readlines()\n",
    "symbLines_TEST = symbFile_TEST.readlines()\n",
    "\n",
    "# Create a dictionary holding the predicted label of the DL model.\n",
    "dict_of_dl_labels_TEST = defaultdict()\n",
    "# Create a dictionary holding the predicted label of the GKR4NLI.\n",
    "dict_of_symb_labels_TEST = defaultdict()\n",
    "# Create a dictionary holding the gold label.\n",
    "dict_of_gold_labels_TEST = defaultdict()\n",
    "\n",
    "\n",
    "# Go through the dl file and store the predicted label of each pair in a dictionary.\n",
    "for line in dlLines_TEST:\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    elements = line.split(\"\\t\")\n",
    "    dict_of_dl_labels_TEST[elements[0]] = elements[1].replace(\"0\", \"E\").replace(\"1\", \"C\").replace(\"2\", \"N\")\n",
    "\n",
    "    \n",
    "# Go through the symbolic file and store the predicted label of GKR4NLI and the gold labels of each pair in a dictionary.\n",
    "for line in symbLines_TEST:\n",
    "    if line.startswith(\"pair_ID\"):\n",
    "        continue\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    elements = line.split(\"\\t\")\n",
    "    id_ = elements[0]\n",
    "    gold_label = elements[1]\n",
    "    dict_of_gold_labels_TEST[id_] = gold_label\n",
    "    symb_label = elements[-1]\n",
    "    dict_of_symb_labels_TEST[id_] = symb_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lenght::  40002\n",
      "Dataset Shape::  (40002, 11)\n",
      "Dataset:: \n",
      "   ID  ComplexCtxs  ContraFlag  Veridical  Antiveridical  Averidical  Equals  \\\n",
      "0   1            0           1          1              0           0       1   \n",
      "1   2            0           0          1              0           0       1   \n",
      "2   3            0           1          1              0           0       1   \n",
      "3   4            0           1          1              0           0       1   \n",
      "4   5            0           1          1              0           0       1   \n",
      "\n",
      "   Superclass  Subclass  Disjoint TargetLabel  \n",
      "0           0         0         1           S  \n",
      "1           0         0         1           B  \n",
      "2           0         0         0           B  \n",
      "3           0         0         1           S  \n",
      "4           0         0         1           S  \n"
     ]
    }
   ],
   "source": [
    "# Read test set to evaluate on it.\n",
    "test_data = pd.read_csv('DAS_test_BERT_GKR4NLI_input_for_hybrid_classifier.csv', sep= '\\t', header= 0)\n",
    "\n",
    "print (\"Dataset Lenght:: \", len(test_data))\n",
    "print (\"Dataset Shape:: \", test_data.shape)\n",
    "\n",
    "print (\"Dataset:: \")\n",
    "print (test_data.head())\n",
    "\n",
    "\n",
    "X_test = test_data.values[:, 1:-1]\n",
    "Y_test = test_data.values[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of correct classifications: 36352\n",
      "Percentage of correct classifications: 0.9087545622718864\n"
     ]
    }
   ],
   "source": [
    "# Predict labels for test set. The predicted labels are one of S, DL or B, expressing the component that the hybrid \n",
    "# classifier predicted to get the inference relation right.\n",
    "predicted = mlp.predict(X_test)\n",
    "# Write the final results into a file for better error-analysis.\n",
    "outputFile = open('DAS_test_hybrid_results.csv', 'w')\n",
    "outputFile.write(\"pair_ID\\tHybridLabel\\tMappedLabel\\tGoldLabel\\n\")\n",
    "\n",
    "i = 0\n",
    "correct = 0\n",
    "for pred in predicted:\n",
    "    test_id = np.array2string(test_data.values[i:i+1,0])\n",
    "    test_id = test_id.replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    #print (test_id)\n",
    "    i += 1\n",
    "    dl_pred = dict_of_dl_labels_TEST[test_id]\n",
    "    symb_pred = dict_of_symb_labels_TEST[test_id]\n",
    "    gold = dict_of_gold_labels_TEST[test_id]\n",
    "    hybrid_pred = \"\"   \n",
    "    # map hybrid prediction to a proper inference label\n",
    "    # if you are using our trained classifier, you have to use the following code: (because we used slighlty different\n",
    "    # abbreviations for each label -- B for BERT, R for rule-based and BR for bert/rule-based) \n",
    "    if pred == \"B\":\n",
    "        hybrid_pred = dl_pred\n",
    "    elif pred == \"R\":\n",
    "        hybrid_pred = symb_pred\n",
    "    elif pred == \"BR\":\n",
    "        hybrid_pred = dl_pred\n",
    "    # If you have trained your own model, please use following code (and abbreviations):\n",
    "    #if pred == \"DL\":\n",
    "    #    hybrid_pred = dl_pred\n",
    "    #elif pred == \"S\":\n",
    "    #    hybrid_pred = symb_pred\n",
    "    #elif pred == \"B\":\n",
    "    #    hybrid_pred = dl_pred        \n",
    "    # Check how many hybrid labels are indeed the correct labels.\n",
    "    #print (hybrid_pred+ \" \"+gold)\n",
    "    outputFile.write(test_id+\"\\t\"+pred+\"\\t\"+hybrid_pred+\"\\t\"+gold+\"\\n\")\n",
    "    if hybrid_pred == gold:\n",
    "        correct += 1\n",
    "\n",
    "print (\"No of correct classifications: \"+str(correct))\n",
    "print (\"Percentage of correct classifications: \"+str(correct/(len(test_data))))\n",
    "\n",
    "outputFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the trained classifier and evaluating on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to evaluate on your own test set, follow these steps:\n",
    "\n",
    "1. Make sure you have run your dataset on GKR4NLI and BERT/XLNet and that you have produced the necessary output files: *dataset_GKR4NLI_results.csv* and *dataset_BERT_results.csv* (see README on how to produce these files).\n",
    "\n",
    "2. In order to produce an accurate test set for the classifier, follow the steps described in the Section above *Producing the training set* to produce the mergedFile.\n",
    "\n",
    "If you just want to reproduce the results, the files of steps 1. and 2. of SICK, DAS and HANS are found in the corresponding folders, so you don't have to do steps 1. and 2. \n",
    "\n",
    "3. You can evaluate on the produced test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. import the trained model\n",
    "import pickle\n",
    "filename = 'mlp_model_trained_on_bert.sav'\n",
    "trained_model = pickle.load(open(filename, 'rb'))\n",
    "mlp = trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Follow all steps detailed in the Section *Evaluating the classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
